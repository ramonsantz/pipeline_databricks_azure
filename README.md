# Pipeline de Dados na Azure com Databricks e Data Factory
<p align="center">
  <img width="580" height="320" alt="Pipeline no Azure Data Factory" src="https://github.com/user-attachments/assets/fb6a3189-3dcc-4e41-99e1-b9e2c2ae585f" />
</p>
Este projeto demonstra a criaÃ§Ã£o e orquestraÃ§Ã£o de um **pipeline de dados em nuvem** utilizando **Azure Databricks** e **Azure Data Factory (ADF)**.  
O objetivo Ã© construir um fluxo automatizado de ingestÃ£o, transformaÃ§Ã£o e armazenamento de dados em um **Data Lake**, aplicando boas prÃ¡ticas de Engenharia de Dados.

---

## VisÃ£o Geral

O pipeline desenvolvido realiza todas as etapas de um fluxo moderno de dados:

- Leitura de um **dataset JSON de imÃ³veis** armazenado na camada **Inbound** (`dados_brutos_imoveis.json`);
- Processamento e transformaÃ§Ã£o dos dados no **Azure Databricks**, gerando as camadas **Bronze** e **Silver**;
- OrquestraÃ§Ã£o e automaÃ§Ã£o das execuÃ§Ãµes com **Azure Data Factory**, utilizando um **gatilho (trigger)** configurado para rodar **a cada 1 hora**.

Esse projeto tem como propÃ³sito **demonstrar o ciclo completo de um pipeline de dados em nuvem**, desde a ingestÃ£o atÃ© o tratamento final dos dados, com monitoramento e automaÃ§Ã£o.

---

## Estrutura de Camadas do Data Lake

Inbound â†’ Bronze â†’ Silver

ruby
Copy code

| Camada | DescriÃ§Ã£o |
|:--------|:-----------|
| **Inbound** | Dados brutos em JSON (dados de imÃ³veis). |
| **Bronze** | Dados padronizados e com limpeza bÃ¡sica. |
| **Silver** | Dados tratados e otimizados em formato Delta, prontos para consumo analÃ­tico. |

---

## âš™ï¸ Tecnologias Utilizadas

| Categoria | Ferramenta | DescriÃ§Ã£o |
|------------|-------------|-----------|
| **Cloud Provider** | Microsoft Azure | Hospedagem e gerenciamento dos serviÃ§os em nuvem |
| **Processamento** | Azure Databricks | Tratamento e transformaÃ§Ã£o dos dados |
| **OrquestraÃ§Ã£o** | Azure Data Factory | Agendamento e automaÃ§Ã£o dos pipelines |
| **Armazenamento** | Azure Data Lake Storage | EstruturaÃ§Ã£o das camadas Inbound, Bronze e Silver |
| **Linguagens** | Python / Scala | Desenvolvimento dos notebooks de ETL |
| **Versionamento** | GitHub | Controle de versÃ£o do cÃ³digo e integraÃ§Ã£o com Databricks |

---

## ğŸ“‚ Estrutura do Projeto

A estrutura do repositÃ³rio no GitHub estÃ¡ organizada da seguinte forma:

ğŸ“ pipeline_databricks_azure/
â”‚
â”œâ”€â”€ ğŸ“ databricks-curso-pipeline/ 
â”œâ”€â”€ ğŸ“ factory/ # DefiniÃ§Ã£o da factory do Azure Data Factory
â”œâ”€â”€ ğŸ“ linkedService/ # ConexÃµes entre ADF, Databricks e Data Lake
â”œâ”€â”€ ğŸ“ notebooks/ # Notebooks de transformaÃ§Ã£o no Databricks
â”‚ â”œâ”€â”€ inbound_to_bronze.py
â”‚ â”œâ”€â”€ bronze_to_silver.py
â”‚ â””â”€â”€ python_etl_inbound_to_silver.py
â”‚
â”œâ”€â”€ ğŸ“ pipeline/ # Pipeline principal do Data Factory
â”‚ â””â”€â”€ datalake_ingestion_pipeline.json
â”‚
â”œâ”€â”€ ğŸ“ trigger/ # Gatilho configurado para execuÃ§Ã£o a cada 1 hora
â”‚ â””â”€â”€ pipeline_trigger.json
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ publish_config.json
â””â”€â”€ README.md

---

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

1. **Dados Iniciais (Inbound):**  
   O arquivo `dados_brutos_imoveis.json` Ã© armazenado na camada *Inbound* do Data Lake.

2. **TransformaÃ§Ã£o no Databricks:**  
   - O notebook `inbound_to_bronze` lÃª e padroniza os dados.  
   - O notebook `bronze_to_silver` aplica transformaÃ§Ãµes adicionais e salva os resultados em **formato Delta** na camada Silver.

3. **OrquestraÃ§Ã£o com Data Factory:**  
   - O pipeline no ADF utiliza duas atividades do tipo **Databricks**, executadas em sequÃªncia.  
   - Um **gatilho (trigger)** foi configurado para executar o pipeline automaticamente **a cada 1 hora**.

4. **Monitoramento:**  
   O status das execuÃ§Ãµes pode ser acompanhado no painel do **Azure Data Factory**, com logs detalhados e histÃ³rico de runs.

---

## ğŸ§  Conceitos Aplicados

- Estrutura de **Data Lake em mÃºltiplas camadas** (Inbound, Bronze, Silver);  
- IngestÃ£o e tratamento de **dados brutos JSON**;  
- Uso do **Delta Lake** para versionamento e otimizaÃ§Ã£o de consultas;  
- **OrquestraÃ§Ã£o e automaÃ§Ã£o** com o Azure Data Factory;  
- Boas prÃ¡ticas de **integraÃ§Ã£o entre Databricks, Data Factory e Data Lake**;  
- **Versionamento com GitHub**, permitindo CI/CD e rastreabilidade.

---

## Pipeline no Data Factory

Fluxo visual do pipeline criado no **Azure Data Factory**:

[ Databricks: ingestao-camada-bronze ] ---> [ Databricks: ingestao-camada-silver ]

yaml
Copy code

Cada notebook Ã© executado em sequÃªncia, processando os dados das camadas anteriores e gravando os resultados tratados no Data Lake.

*(Na imagem abaixo, ambas as atividades aparecem com status â€œâœ”ï¸ Bem-sucedidoâ€ apÃ³s a execuÃ§Ã£o do pipeline.)*

---

## ğŸ•’ AutomaÃ§Ã£o com Gatilho

O **gatilho (trigger)** do ADF executa automaticamente o pipeline a cada **1 hora**, garantindo a atualizaÃ§Ã£o contÃ­nua dos dados.  
Essa automaÃ§Ã£o elimina a necessidade de execuÃ§Ã£o manual, mantendo o fluxo sempre atualizado.

---

## ğŸ§¾ Resultados

- Dados brutos de imÃ³veis transformados em formato Delta otimizado;  
- Pipeline automatizado e monitorado via ADF;  
- Estrutura modular e escalÃ¡vel, facilitando futuras extensÃµes;  
- Base sÃ³lida para construÃ§Ã£o de camadas **Gold** e relatÃ³rios em BI.


---

## ğŸ‘¨â€ğŸ’» Autor

**Ramon Santos**  
Projeto pessoal desenvolvido para prÃ¡tica e portfÃ³lio voltado Ã  **Engenharia de Dados**  
ğŸ“§ Contato: [LinkedIn](#)
